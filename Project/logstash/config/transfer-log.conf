# Input section for NGINX logs
input {
  file {
    path => "/home/mekky/Project/logs/preprocessed/nginx/output.log"  # Path to preprocessed NGINX log files
    start_position => "beginning"  # Read from the beginning only for the first time
    sincedb_path => "/home/mekky/Project/sincedb/nginx_sincedb"  # Persistent sincedb_path for NGINX logs
    type => "nginx_log"  # Set a type to differentiate logs
  }

  # Input section for MySQL logs
  file {
    path => "/home/mekky/Project/logs/preprocessed/mysql/output.log"  # Path to preprocessed MySQL log files
    start_position => "beginning"  # Read from the beginning only for the first time
    sincedb_path => "/home/mekky/Project/sincedb/mysql_sincedb"  # Persistent sincedb_path for MySQL logs
    type => "mysql_log"  # Set a type to differentiate logs
  }

  # Input section for Firewall (Syslog) logs
  file {
    path => "/home/mekky/Project/logs/preprocessed/firewall/*"  # Path to preprocessed firewall log files
    start_position => "beginning"  # Read from the beginning only for the first time
    sincedb_path => "/home/mekky/Project/sincedb/firewall_sincedb"  # Persistent sincedb_path for firewall logs
    type => "firewall_log"  # Set a type to differentiate logs
  }
}

# Filter section to process logs and extract useful fields
filter {
  # Grok pattern to parse NGINX logs
  if [type] == "nginx_log" {
    grok {
      match => { 
        "message" => "%{IPORHOST:client_ip} %{HTTPDATE:timestamp} %{WORD:method} %{URIPATHPARAM:request} %{NOTSPACE:http_version} %{NUMBER:status_code} %{NUMBER:response_size} %{NOTSPACE:referrer} %{GREEDYDATA:user_agent}"
      }
    }

    # Add event_type for NGINX logs
    mutate {
      add_field => { "event_type" => "%{type}" }
      rename => { "request" => "url" }
    }

    # Date filter to parse the timestamp field for NGINX logs
    date {
      match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
      target => "@timestamp"
    }

    # Remove the original message field after extraction
    mutate {
      remove_field => ["message"]
    }

    mutate {
      add_field => { 
        "formatted_message" => "%{event_type}|%{client_ip}|%{method}|%{url}|%{status_code}|%{response_size}|%{referrer}|%{user_agent}" 
      }
    }
  }

  # Grok pattern to parse MySQL logs
  if [type] == "mysql_log" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{GREEDYDATA:sql_query}" 
      }
    }

    # Add event_type for MySQL logs
    mutate {
      add_field => { "event_type" => "%{type}" }
    }

    # Date filter to parse the timestamp field for MySQL logs
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
    }

    # Remove the original message field after extraction
    mutate {
      remove_field => ["message"]
    }

    mutate {
      add_field => { 
        "formatted_message" => "%{event_type}|%{sql_query}" 
      }
    }
  }
}

# Output section to handle different types of logs
output {

  # Combine all types into a single output
  tcp {
    host => "192.168.1.64"
    port => 5045
    codec => line {
      format => "%{formatted_message}"
    }
  }

  # Debugging output for all logs
  stdout { codec => rubydebug }  # Debugging output
}