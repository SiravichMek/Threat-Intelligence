1) cd Project
2) sudo docker-compose up --build --scale app=3
3) ~/kafka_2.13-3.0.0/bin/zookeeper-server-start.sh ~/kafka_2.13-3.0.0/config/zookeeper.properties
(Start ZooKeeper)
4) ~/kafka_2.13-3.0.0/bin/kafka-server-start.sh ~/kafka_2.13-3.0.0/config/server.properties
(Start Kafka)
5) sudo service filebeat start
(Start FileBeat)
6) sudo service logstash start
(Start Logstash)
7) spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 spark/log_preprocessing.py
(Execute PySpark Script)
 **Note** Every time to start pipeline need to recreate the checkpoint directory and delete output.log


Note:
1. Checking Topic Kafka:
	1.1 cd kafka_2.13-3.0.0/bin
	1.2 ./kafka-topics.sh --list --bootstrap-server localhost:9092
2. Modify Beat Configuration file:
	2.1 nano /etc/filebeat/filebeat.yml
3. Kafka Create Topic:
	3.1 ./kafka-topics.sh --create --topic nginx-logs --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
4. Validate Logstash Configuration:
	logstash -f D:\ELK_stack\logstash-8.15.2\config\logstash-sample.conf --config.test_and_exit
5. DDoS ICMP:
	1..20 | ForEach-Object { Start-Process ping -ArgumentList "-n 1 192.168.1.40" }
6. Get Realtime ICMP log data:
	sudo tcpdump -i enp0s3 icmp
7. Check data in topic:
	./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic nginx-logs --from-
beginning
8. Filebeat Log Checking:
	sudo tail -f /var/log/filebeat/filebeat
9. Logstash Pipeline Check:
	sudo tail -f /var/log/logstash/logstash-plain.log

